{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802001fd-b174-4817-ac3c-453876b8f473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\roger\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\roger\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\roger\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\roger\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\roger\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\roger\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9850640-e71c-4625-b864-765c7d04a086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8b75663-7828-420f-9f04-06edfe0ad36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing is fun.Let's learn tokenization today!Tokenization splits text into pieces.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f98f288-4db1-4818-8b94-527a169be9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\roger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a05b0035-a3c6-41c6-b007-2eaccdd6da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tokenization from sentence to paragraph\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad069704-9c71-4ef1-987d-2323fc3b4303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing is fun.', \"Let's learn tokenization today!\", 'Tokenization splits text into pieces.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "import nltk\n",
    "\n",
    "corpus=\"Natural Language Processing is fun. Let's learn tokenization today! Tokenization splits text into pieces.\"\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "print(tokenizer.tokenize(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bce2184-d320-4da2-a681-618812df2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ac9e88b-4ea5-466b-9f25-8cce65195161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c99bab55-cd9c-4dd6-ab12-8e3ea19d6b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing is fun.\n",
      "Let's learn tokenization today!\n",
      "Tokenization splits text into pieces.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f6622b8-3cd1-49c7-9d2e-def0de8a9f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-Punct Tokens: ['Natural', 'Language', 'Processing', 'is', 'amazing', '!', 'Let', \"'\", 's', 'learn', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "text = \"Natural Language Processing is amazing! Let's learn tokenization.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Word-Punct Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a45d09-2d7b-4642-92f6-8157f4ce183a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
